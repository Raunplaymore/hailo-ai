{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2937801,"sourceType":"datasetVersion","datasetId":1801190},{"sourceId":2938397,"sourceType":"datasetVersion","datasetId":1801557},{"sourceId":82597268,"sourceType":"kernelVersion"}],"dockerImageVersionId":30154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Golf Positions\n\n## Introduction\n\nTaken from https://github.com/wmcnally/golfdb and shown in the paper here https://arxiv.org/abs/1903.06528\n\nThe code separates the golf swing into a number of different segments based on body and golf club positions.\n\n\nUpload golfdb3 for the basics or golfdb2/golfdb for the full code and golf videos. golfdb3 has the model and it's weights and a test video.","metadata":{}},{"cell_type":"markdown","source":"## Specify the file to use\nAdd downloaded directory (not always necsessary) and specify the video file.","metadata":{}},{"cell_type":"code","source":"!cp -r ../input/golfdb3/* ./\n\nstra='../input/golfdb3/test_video.mp4'\nstra='../input/golfdb2/golfdb/data/videos_160/1017.mp4'\nstra","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-17T14:41:47.470986Z","iopub.execute_input":"2021-12-17T14:41:47.471268Z","iopub.status.idle":"2021-12-17T14:41:48.336043Z","shell.execute_reply.started":"2021-12-17T14:41:47.471237Z","shell.execute_reply":"2021-12-17T14:41:48.335265Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports, classes and defs\nSome imports. Neural nets using Torch","metadata":{}},{"cell_type":"code","source":"import scipy.io\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n# from eval import ToTensor, Normalize\n# from model import EventDetector\nimport numpy as np\nimport torch.nn.functional as F\nimport cv2\nfrom torch.autograd import Variable","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:22:11.010559Z","iopub.execute_input":"2021-12-17T14:22:11.011282Z","iopub.status.idle":"2021-12-17T14:22:11.579887Z","shell.execute_reply.started":"2021-12-17T14:22:11.011239Z","shell.execute_reply":"2021-12-17T14:22:11.579132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"These classes and definitions are taken from the files in the GitHub directory","metadata":{}},{"cell_type":"code","source":"class SampleVideo(Dataset):\n    def __init__(self, path, input_size=160, transform=None):\n        self.path = path\n        self.input_size = input_size\n        self.transform = transform\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, idx):\n        cap = cv2.VideoCapture(self.path)\n        frame_size = [cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(cv2.CAP_PROP_FRAME_WIDTH)]\n        ratio = self.input_size / max(frame_size)\n        new_size = tuple([int(x * ratio) for x in frame_size])\n        delta_w = self.input_size - new_size[1]\n        delta_h = self.input_size - new_size[0]\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\n\n        # preprocess and return frames\n        images = []\n        for pos in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n            _, img = cap.read()\n            resized = cv2.resize(img, (new_size[1], new_size[0]))\n            b_img = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT,\n                                       value=[0.406 * 255, 0.456 * 255, 0.485 * 255])  # ImageNet means (BGR)\n\n            b_img_rgb = cv2.cvtColor(b_img, cv2.COLOR_BGR2RGB)\n            images.append(b_img_rgb)\n        cap.release()\n        labels = np.zeros(len(images)) # only for compatibility with transforms\n        sample = {'images': np.asarray(images), 'labels': np.asarray(labels)}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:22:14.460455Z","iopub.execute_input":"2021-12-17T14:22:14.461381Z","iopub.status.idle":"2021-12-17T14:22:14.473474Z","shell.execute_reply.started":"2021-12-17T14:22:14.461343Z","shell.execute_reply":"2021-12-17T14:22:14.472813Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n    def __call__(self, sample):\n        images, labels = sample['images'], sample['labels']\n        images = images.transpose((0, 3, 1, 2))\n        return {'images': torch.from_numpy(images).float().div(255.),\n                'labels': torch.from_numpy(labels).long()}\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = torch.tensor(mean, dtype=torch.float32)\n        self.std = torch.tensor(std, dtype=torch.float32)\n\n    def __call__(self, sample):\n        images, labels = sample['images'], sample['labels']\n        images.sub_(self.mean[None, :, None, None]).div_(self.std[None, :, None, None])\n        return {'images': images, 'labels': labels}\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:22:17.905172Z","iopub.execute_input":"2021-12-17T14:22:17.905431Z","iopub.status.idle":"2021-12-17T14:22:17.914177Z","shell.execute_reply.started":"2021-12-17T14:22:17.905403Z","shell.execute_reply":"2021-12-17T14:22:17.913384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport math\n\n\"\"\"\nhttps://github.com/tonylins/pytorch-mobilenet-v2\n\"\"\"\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        min_depth = 16\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult) if width_mult >= 1.0 else input_channel\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = max(int(c * width_mult), min_depth)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:22:20.737806Z","iopub.execute_input":"2021-12-17T14:22:20.73842Z","iopub.status.idle":"2021-12-17T14:22:20.765185Z","shell.execute_reply.started":"2021-12-17T14:22:20.738385Z","shell.execute_reply":"2021-12-17T14:22:20.764471Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nclass EventDetector(nn.Module):\n    def __init__(self, pretrain, width_mult, lstm_layers, lstm_hidden, bidirectional=True, dropout=True):\n        super(EventDetector, self).__init__()\n        self.width_mult = width_mult\n        self.lstm_layers = lstm_layers\n        self.lstm_hidden = lstm_hidden\n        self.bidirectional = bidirectional\n        self.dropout = dropout\n\n        net = MobileNetV2(width_mult=width_mult)\n        state_dict_mobilenet = torch.load('mobilenet_v2.pth.tar')\n        if pretrain:\n            net.load_state_dict(state_dict_mobilenet)\n\n        self.cnn = nn.Sequential(*list(net.children())[0][:19])\n        self.rnn = nn.LSTM(int(1280*width_mult if width_mult > 1.0 else 1280),\n                           self.lstm_hidden, self.lstm_layers,\n                           batch_first=True, bidirectional=bidirectional)\n        if self.bidirectional:\n            self.lin = nn.Linear(2*self.lstm_hidden, 9)\n        else:\n            self.lin = nn.Linear(self.lstm_hidden, 9)\n        if self.dropout:\n            self.drop = nn.Dropout(0.5)\n\n    def init_hidden(self, batch_size):\n        if self.bidirectional:\n            return (Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n                    Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n        else:\n            return (Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n                    Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n\n    def forward(self, x, lengths=None):\n        batch_size, timesteps, C, H, W = x.size()\n        self.hidden = self.init_hidden(batch_size)\n\n        # CNN forward\n        c_in = x.view(batch_size * timesteps, C, H, W)\n        c_out = self.cnn(c_in)\n        c_out = c_out.mean(3).mean(2)\n        if self.dropout:\n            c_out = self.drop(c_out)\n\n        # LSTM forward\n        r_in = c_out.view(batch_size, timesteps, -1)\n        r_out, states = self.rnn(r_in, self.hidden)\n        out = self.lin(r_out)\n        out = out.view(batch_size*timesteps,9)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:22:26.608653Z","iopub.execute_input":"2021-12-17T14:22:26.60943Z","iopub.status.idle":"2021-12-17T14:22:26.62381Z","shell.execute_reply.started":"2021-12-17T14:22:26.609393Z","shell.execute_reply":"2021-12-17T14:22:26.623089Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run it","metadata":{}},{"cell_type":"code","source":"seq_length=64\n\nds = SampleVideo(stra, transform=transforms.Compose([ToTensor(),\n                                Normalize([0.485, 0.456, 0.406],\n                                          [0.229, 0.224, 0.225])]))\n\ndl = DataLoader(ds, batch_size=1, shuffle=False, drop_last=False)\n\nmodel = EventDetector(pretrain=True,\n                      width_mult=1.,\n                      lstm_layers=1,\n                      lstm_hidden=256,\n                      bidirectional=True,\n                      dropout=False)\ntry:\n    save_dict = torch.load('models/swingnet_1800.pth.tar')\nexcept:\n    print(\"Model weights not found. Download model weights and place in 'models' folder. See README for instructions\")\n    \n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nmodel.load_state_dict(save_dict['model_state_dict'])\nmodel.to(device)\nmodel.eval()\nprint(\"Loaded model weights\")\n\nprint('Testing...')\nfor sample in dl:\n    images = sample['images']\n    # full samples do not fit into GPU memory so evaluate sample in 'seq_length' batches\n    batch = 0\n    while batch * seq_length < images.shape[1]:\n        if (batch + 1) * seq_length > images.shape[1]:\n            image_batch = images[:, batch * seq_length:, :, :, :]\n        else:\n            image_batch = images[:, batch * seq_length:(batch + 1) * seq_length, :, :, :]\n        logits = model(image_batch.cuda())\n        if batch == 0:\n            probs = F.softmax(logits.data, dim=1).cpu().numpy()\n        else:\n            probs = np.append(probs, F.softmax(logits.data, dim=1).cpu().numpy(), 0)\n        batch += 1\n\n        \nevents = np.argmax(probs, axis=0)[:-1]\nprint('Predicted event frames: {}'.format(events))\n\n\nconfidence = []\nfor i, e in enumerate(events):\n    confidence.append(probs[e, i])\nprint('Confidence: {}'.format([np.round(c, 3) for c in confidence]))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:42:00.801794Z","iopub.execute_input":"2021-12-17T14:42:00.802381Z","iopub.status.idle":"2021-12-17T14:42:01.698349Z","shell.execute_reply.started":"2021-12-17T14:42:00.802343Z","shell.execute_reply":"2021-12-17T14:42:01.697492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot the results","metadata":{}},{"cell_type":"code","source":"import os\n##delte images\nlsa=os.listdir()\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\n# print(fimg)\nimgs=[os.remove(ff) for ff in fimg]\n\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\nprint(fimg)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:42:07.42086Z","iopub.execute_input":"2021-12-17T14:42:07.421687Z","iopub.status.idle":"2021-12-17T14:42:07.429234Z","shell.execute_reply.started":"2021-12-17T14:42:07.421648Z","shell.execute_reply":"2021-12-17T14:42:07.428207Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def createImages(fila,pos,nomS):\n    ''' \n    Given a video file location (fila) it will save as images to a folder\n    Given positions in video (pos) these images from the video are saved\n    pos is created based on positions of swings\n    '''\n    import cv2\n    cap = cv2.VideoCapture(fila)\n    eventNom=[0,1,2,3,4,5,6,7]\n    for i, e in enumerate(events):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n        _, img = cap.read()\n        cv2.imwrite(os.path.join(os.getcwd(),'_'+ nomS+'_'+\"frame{:d}.jpg\".format(eventNom[i])), img)     # save frame as JPG file\n    \n    \nfila=stra\npos=events\ncreateImages(fila,pos,'10')","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:42:09.197925Z","iopub.execute_input":"2021-12-17T14:42:09.19832Z","iopub.status.idle":"2021-12-17T14:42:09.267013Z","shell.execute_reply.started":"2021-12-17T14:42:09.198287Z","shell.execute_reply":"2021-12-17T14:42:09.266288Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nlsa=os.listdir()\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\nfimg.sort()\n\nimgs=[mpimg.imread(ff) for ff in fimg]\n\nfimg\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:42:11.825275Z","iopub.execute_input":"2021-12-17T14:42:11.825526Z","iopub.status.idle":"2021-12-17T14:42:11.84303Z","shell.execute_reply.started":"2021-12-17T14:42:11.825496Z","shell.execute_reply":"2021-12-17T14:42:11.842221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cap = cv2.VideoCapture(stra)\n\n\n# plt.subplot(4,2,1)\nf, axs = plt.subplots(4,2,figsize=(15,15))\nfor i, e in enumerate(events):\n    cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n    _, img = cap.read()\n    plt.subplot(4,2,i+1)\n    plt.imshow(img)\n    plt.title(e)","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:42:15.902916Z","iopub.execute_input":"2021-12-17T14:42:15.903172Z","iopub.status.idle":"2021-12-17T14:42:16.967915Z","shell.execute_reply.started":"2021-12-17T14:42:15.903143Z","shell.execute_reply":"2021-12-17T14:42:16.967215Z"},"trusted":true},"outputs":[],"execution_count":null}]}